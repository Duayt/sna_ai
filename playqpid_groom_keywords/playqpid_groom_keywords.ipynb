{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset contains 155 lines\n",
      "‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ù‡∏±‡∏ô ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ô‡∏≥‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡πÇ‡∏•‡∏Å‡∏´‡∏°‡∏∏‡∏ô‡∏£‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á ‡∏£‡∏±‡∏Å‡∏´‡∏°‡∏≤ ‡∏£‡∏±‡∏Å‡∏™‡∏±‡∏ï‡∏ß‡πå (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏ä‡∏≤‡∏•‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏∞ 555) ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå ‡πÑ‡∏°‡πà‡∏™‡∏π‡∏ö‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà ‡πÑ‡∏°‡πà‡πÄ‡∏à‡πâ‡∏≤‡∏ä‡∏π‡πâ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏±‡∏ô ‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏°‡∏µ‡∏ú‡∏• ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ‡∏î‡πâ‡∏≤‡∏ô (‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏∂‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏≤‡∏á‡πÉ‡∏à ‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô) ‡∏Ñ‡πà‡∏∞\n",
      "\n",
      "#‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡∏π‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏µ‡πä‡∏¢‡∏á!! ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ô‡∏µ‡πâ‡∏Ñ‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏≠‡πÉ‡∏™‡πà‡∏ä‡∏∏‡∏î‡πÄ‡∏à‡πâ‡∏≤‡∏™‡∏≤‡∏ß‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏î‡∏µ\n",
      "\n",
      "‡∏õ.‡∏•. ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏ô‡∏π‡∏°‡∏µ‡πÑ‡∏õ‡πÄ‡∏≠‡∏á (‡πÅ‡∏ô‡∏ö‡∏£‡∏π‡∏õ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞) üê∂‚ù§Ô∏è\n",
      "\n",
      "‡∏™‡πÄ‡∏õ‡πá‡∏Ñ‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß : ‡∏Ç‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏î‡∏π‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡∏¢‡∏¥‡πâ‡∏°‡∏°‡∏µ‡πÄ‡∏™‡∏ô‡πà‡∏´‡πå ‡∏™‡∏π‡∏á178+ ‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏î‡∏µ‡∏ã‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ï‡∏¢‡πå ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏àüòö\n",
      "\n",
      "‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß : ‡∏™‡∏π‡∏á 170 ‡∏Ç‡∏∂‡πâ‡∏ô\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read file line by line to list\n",
    "data_list = []\n",
    "\n",
    "dataset_path = './dataset/playqpid_groom_types.txt'\n",
    "with open(dataset_path) as fp:\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "    \n",
    "        data_list.append(line)\n",
    "    \n",
    "        # read next line\n",
    "        line = fp.readline()\n",
    "\n",
    "print ('dataset contains %d lines' % (len(data_list)))\n",
    "\n",
    "for i in range(5):\n",
    "    print (data_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation and whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset contains 155 lines\n",
      "‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏ù‡∏±‡∏ô ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ô‡∏≥‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡πÇ‡∏•‡∏Å‡∏´‡∏°‡∏∏‡∏ô‡∏£‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á ‡∏£‡∏±‡∏Å‡∏´‡∏°‡∏≤ ‡∏£‡∏±‡∏Å‡∏™‡∏±‡∏ï‡∏ß‡πå ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏ä‡∏≤‡∏•‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏∞ 555 ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå ‡πÑ‡∏°‡πà‡∏™‡∏π‡∏ö‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà ‡πÑ‡∏°‡πà‡πÄ‡∏à‡πâ‡∏≤‡∏ä‡∏π‡πâ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏±‡∏ô ‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏°‡∏µ‡∏ú‡∏• ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ‡∏î‡πâ‡∏≤‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏∂‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏≤‡∏á‡πÉ‡∏à ‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô ‡∏Ñ‡πà‡∏∞\n",
      "‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡∏π‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏µ‡πä‡∏¢‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ô‡∏µ‡πâ‡∏Ñ‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏≠‡πÉ‡∏™‡πà‡∏ä‡∏∏‡∏î‡πÄ‡∏à‡πâ‡∏≤‡∏™‡∏≤‡∏ß‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏î‡∏µ\n",
      "‡∏õ‡∏• ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏ô‡∏π‡∏°‡∏µ‡πÑ‡∏õ‡πÄ‡∏≠‡∏á ‡πÅ‡∏ô‡∏ö‡∏£‡∏π‡∏õ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞ üê∂‚ù§Ô∏è\n",
      "‡∏™‡πÄ‡∏õ‡πá‡∏Ñ‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß  ‡∏Ç‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏î‡∏π‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡∏¢‡∏¥‡πâ‡∏°‡∏°‡∏µ‡πÄ‡∏™‡∏ô‡πà‡∏´‡πå ‡∏™‡∏π‡∏á178 ‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏î‡∏µ‡∏ã‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ï‡∏¢‡πå ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏àüòö\n",
      "‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß  ‡∏™‡∏π‡∏á 170 ‡∏Ç‡∏∂‡πâ‡∏ô\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "data_list_no_punc = []\n",
    "\n",
    "for line in data_list:\n",
    "    no_punc = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    clean = no_punc.rstrip()\n",
    "    \n",
    "    data_list_no_punc.append(clean)\n",
    "    \n",
    "print ('dataset contains %d lines' % (len(data_list_no_punc)))\n",
    "\n",
    "for i in range(5):\n",
    "    print (data_list_no_punc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize using Deepcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832211f39ea34acfa77d7d28a45fb29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0914 07:49:39.928745 4520342976 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0914 07:49:39.963252 4520342976 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0914 07:49:39.972306 4520342976 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0914 07:49:40.009952 4520342976 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0914 07:49:40.022058 4520342976 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0914 07:49:40.926941 4520342976 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0914 07:49:40.934358 4520342976 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0914 07:49:40.941681 4520342976 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset contains 155 lines\n",
      "‡∏™‡πÄ‡∏õ‡∏Ñ ‡πÄ‡∏à‡πâ‡∏≤ ‡∏ö‡πà‡∏≤‡∏ß ‡πÉ‡∏ô ‡∏ù‡∏±‡∏ô ‡∏Ñ‡∏∑‡∏≠ ‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô ‡∏ú‡∏π‡πâ ‡∏ô‡∏≥ ‡πÅ‡∏ï‡πà ‡πÑ‡∏°‡πà ‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏ö‡∏ö ‡πÇ‡∏•‡∏Å ‡∏´‡∏°‡∏∏‡∏ô ‡∏£‡∏≠‡∏ö ‡∏ï‡∏±‡∏ß ‡πÄ‡∏≠‡∏á ‡∏£‡∏±‡∏Å‡∏´‡∏°‡∏≤ ‡∏£‡∏±‡∏Å ‡∏™‡∏±‡∏ï‡∏ß‡πå ‡πÅ‡∏ï‡πà ‡πÑ‡∏°‡πà ‡πÄ‡∏≠‡∏≤ ‡∏Ç‡∏ô‡∏≤‡∏î ‡∏ä‡∏≤‡∏•‡∏µ ‡∏ô‡∏∞ ‡∏Ñ‡∏∞ 555 ‡πÑ‡∏°‡πà ‡∏ï‡∏¥‡∏î ‡∏Å‡∏≤‡∏£ ‡∏û‡∏ô‡∏±‡∏ô ‡πÑ‡∏°‡πà ‡∏ï‡∏¥‡∏î ‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå ‡πÑ‡∏°‡πà ‡∏™‡∏π‡∏ö ‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà ‡πÑ‡∏°‡πà ‡πÄ‡∏à‡πâ‡∏≤‡∏ä‡∏π‡πâ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‡∏Å‡∏±‡∏ô ‡∏°‡∏µ ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• ‡∏°‡∏µ ‡∏ú‡∏• ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡πÄ‡∏£‡∏≤ ‡πÑ‡∏î‡πâ ‡πÉ‡∏ô ‡∏ó‡∏∏‡∏Å ‡πÜ ‡∏î‡πâ‡∏≤‡∏ô ‡πÄ‡∏õ‡πá‡∏ô ‡∏ó‡∏µ‡πà‡∏û‡∏∂‡πà‡∏á ‡πÑ‡∏î‡πâ ‡∏ó‡∏±‡πâ‡∏á ‡∏ó‡∏≤‡∏á ‡πÉ‡∏à ‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞ ‡∏ó‡∏≤‡∏á ‡∏Å‡∏≤‡∏£ ‡πÄ‡∏á‡∏¥‡∏ô ‡∏Ñ‡πà‡∏∞\n",
      "‡∏Ç‡∏≠ ‡πÉ‡∏´‡πâ ‡∏´‡∏ô‡∏π ‡πÑ‡∏î‡πâ ‡πÄ‡∏û‡∏µ‡πä‡∏¢‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï ‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏á ‡πÑ‡∏°‡πà ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏á‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏≠ ‡πÉ‡∏™‡πà ‡∏ä‡∏∏‡∏î ‡πÄ‡∏à‡πâ‡∏≤ ‡∏™‡∏≤‡∏ß ‡∏ñ‡πà‡∏≤‡∏¢ ‡∏£‡∏π‡∏õ ‡∏™‡∏±‡∏Å ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏Å‡πá ‡∏¢‡∏±‡∏á ‡∏î‡∏µ\n",
      "‡∏õ‡∏• ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ ‡πÄ‡∏à‡πâ‡∏≤ ‡∏ö‡πà‡∏≤‡∏ß ‡πÑ‡∏°‡πà ‡πÑ‡∏î‡πâ ‡∏´‡∏ô‡∏π ‡∏°‡∏µ ‡πÑ‡∏õ ‡πÄ‡∏≠‡∏á ‡πÅ‡∏ô‡∏ö ‡∏£‡∏π‡∏õ ‡∏°‡∏≤ ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ñ‡πà‡∏∞ üê∂ ‚ù§ Ô∏è\n",
      "‡∏™‡πÄ‡∏õ‡πá‡∏Ñ ‡πÄ‡∏à‡πâ‡∏≤ ‡∏ö‡πà‡∏≤‡∏ß ‡∏Ç‡∏≠ ‡πÄ‡∏õ‡πá‡∏ô ‡∏ú‡∏π‡πâ ‡∏ä‡∏≤‡∏¢ ‡∏ó‡∏µ‡πà ‡∏î‡∏π ‡∏†‡∏≤‡∏¢ ‡∏ô‡∏≠‡∏Å ‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡∏¢‡∏¥‡πâ‡∏° ‡∏°‡∏µ ‡πÄ‡∏™‡∏ô‡πà‡∏´‡πå ‡∏™‡∏π‡∏á 178 ‡πÅ‡∏•‡∏∞ ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡∏ó‡πâ‡∏≤‡∏¢ ‡∏™‡∏∏‡∏î ‡∏Ç‡∏≠ ‡πÉ‡∏´‡πâ ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ‡∏ô ‡∏î‡∏µ ‡∏ã‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ï‡∏¢‡πå ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏à üòö\n",
      "‡∏™‡πÄ‡∏õ‡∏Ñ ‡πÄ‡∏à‡πâ‡∏≤ ‡∏ö‡πà‡∏≤‡∏ß ‡∏™‡∏π‡∏á 170 ‡∏Ç‡∏∂‡πâ‡∏ô\n"
     ]
    }
   ],
   "source": [
    "import deepcut\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "data_list_deepcut = []\n",
    "\n",
    "for line in tqdm_notebook(data_list_no_punc):\n",
    "    token_list = deepcut.tokenize(line)\n",
    "    \n",
    "    temp_list = []\n",
    "    for it in token_list:\n",
    "        if it.isspace():\n",
    "            continue\n",
    "        temp_list.append(it)\n",
    "    \n",
    "    deepcut_line = \" \".join(temp_list)\n",
    "    data_list_deepcut.append(deepcut_line)\n",
    "\n",
    "print ('dataset contains %d lines' % (len(data_list_deepcut)))\n",
    "\n",
    "for i in range(5):\n",
    "    print (data_list_deepcut[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target wordcloud LIB, JSON file writer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "import json\n",
    "\n",
    "def write_json_file(word_counter, file_name):\n",
    "    # convert to list and export\n",
    "    wordcloud_list = []\n",
    "    for key in word_counter:\n",
    "        obj = {}\n",
    "        obj['x'] = key\n",
    "        obj['value'] = word_counter[key]\n",
    "        wordcloud_list.append(obj)\n",
    "\n",
    "    wordcloud_list\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(json.dumps(wordcloud_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 1-gram word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65774efd8992489c9855d881961354f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_counter = {}\n",
    "\n",
    "for line in tqdm_notebook(data_list_deepcut):\n",
    "    \n",
    "    token_list = line.split(' ')\n",
    "    \n",
    "    for token in token_list:\n",
    "        if token not in word_counter.keys():\n",
    "            word_counter[token] = 0\n",
    "        word_counter[token] += 1\n",
    "        \n",
    "write_json_file(word_counter, './output/word_cloud_fmt_1_gram.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 2-gram word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b439fadd07445ebcfcb0d759b563bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_counter_2 = {}\n",
    "\n",
    "for line in tqdm_notebook(data_list_deepcut):\n",
    "    \n",
    "    token_list = line.split(' ')\n",
    "    \n",
    "    prev_word = ''\n",
    "    \n",
    "    for i in range(len(token_list)):\n",
    "        if i == 0:\n",
    "            prev_word = token_list[i]\n",
    "            continue\n",
    "        \n",
    "        token = prev_word + token_list[i]\n",
    "        \n",
    "        prev_word = token_list[i]\n",
    "        \n",
    "        if token not in word_counter_2.keys():\n",
    "            word_counter_2[token] = 0\n",
    "        word_counter_2[token] += 1\n",
    "        \n",
    "del word_counter_2['‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß']\n",
    "\n",
    "write_json_file(word_counter_2, './output/word_cloud_fmt_2_grams.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 2-gram word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56e02c2910841acb092cdae79afe2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_counter_3 = {}\n",
    "\n",
    "for line in tqdm_notebook(data_list_deepcut):\n",
    "    \n",
    "    token_list = line.split(' ')\n",
    "    \n",
    "    prev_word = ''\n",
    "    prev_prev_word = ''\n",
    "    \n",
    "    for i in range(len(token_list)):\n",
    "        if i == 0:\n",
    "            prev_word = token_list[i]\n",
    "            continue\n",
    "        if i == 1:\n",
    "            prev_prev_word = prev_word\n",
    "            prev_word = token_list[i]\n",
    "        \n",
    "        token = prev_prev_word + prev_word + token_list[i]\n",
    "        \n",
    "        prev_prev_word = prev_word\n",
    "        prev_word = token_list[i]\n",
    "        \n",
    "        if token not in word_counter_3.keys():\n",
    "            word_counter_3[token] = 0\n",
    "        word_counter_3[token] += 1\n",
    "        \n",
    "# Remove some big words\n",
    "del word_counter_3['‡πÄ‡∏à‡πâ‡∏≤‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß']\n",
    "del word_counter_3['‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏à‡πâ‡∏≤‡πÄ‡∏à‡πâ‡∏≤']\n",
    "del word_counter_3['‡πÄ‡∏à‡πâ‡∏≤‡∏ö‡πà‡∏≤‡∏ß‡∏Ç‡∏≠']\n",
    "del word_counter_3['‡πÉ‡∏™‡πà‡∏ä‡∏∏‡∏î‡πÄ‡∏à‡πâ‡∏≤']\n",
    "\n",
    "write_json_file(word_counter_3, './output/word_cloud_fmt_3_grams.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
